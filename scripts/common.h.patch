--- common.h.orig	2025-01-07 13:18:14
+++ common.h	2025-01-07 13:18:16
@@ -41,6 +41,17 @@

 struct common_control_vector_load_info;

+#define print_build_info() do {                                                                     \
+    fprintf(stderr, "%s: build = %d (%s)\n", __func__, LLAMA_BUILD_NUMBER, LLAMA_COMMIT);           \
+    fprintf(stderr, "%s: built with %s for %s\n", __func__, LLAMA_COMPILER, LLAMA_BUILD_TARGET);    \
+} while(0)
+
+// build info
+extern int LLAMA_BUILD_NUMBER;
+extern char const *LLAMA_COMMIT;
+extern char const *LLAMA_COMPILER;
+extern char const *LLAMA_BUILD_TARGET;
+
 //
 // CPU utils
 //
@@ -181,6 +192,7 @@
 };

 struct common_params {
+    bool vocab_only               = false;
     int32_t n_predict             =    -1; // new tokens to predict
     int32_t n_ctx                 =  4096; // context size
     int32_t n_batch               =  2048; // logical batch size for prompt processing (must be >=32 to use BLAS)
@@ -298,6 +310,9 @@
     bool warmup            = true;  // warmup run
     bool check_tensors     = false; // validate tensor data

+    llama_progress_callback progress_callback = nullptr;
+    void * progress_callback_user_data = nullptr;
+
     lm_ggml_type cache_type_k = LM_GGML_TYPE_F16; // KV cache data type for the K
     lm_ggml_type cache_type_v = LM_GGML_TYPE_F16; // KV cache data type for the V

